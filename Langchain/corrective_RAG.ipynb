{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560cba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a259f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"my_notes.txt\")\n",
    "docs=loader.load()\n",
    "splitter=CharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93357917",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "vectorstore=FAISS.from_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f263ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "vectorstore.save_local(\"selfrag_db\")\n",
    "llm=ChatOpenAI(temperature=0,openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13b1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the RAG chain using LCEL (LangChain Expression Language - modern approach)\n",
    "template = \"\"\"Answer the question based only on the following context:{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aff0c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      " original answer: The largest cat is the Siberian tiger, also known as the Amur tiger, which can weigh up to 660 pounds and grow up to 11 feet in length.\n",
      "corrected using documents: None (no correction needed)\n"
     ]
    }
   ],
   "source": [
    "def corrective_rag(question, top_k=4):\n",
    "    \"\"\"\n",
    "    Runs a first-pass LLM answer, and if low confidence, runs a RAG correction:\n",
    "      - builds a small FAISS DB from docs\n",
    "      - retrieves top_k docs\n",
    "      - composes a prompt with the retrieved context and asks the LLM to answer only using that context\n",
    "    Returns a string formatted as:\n",
    "      original answer: <first_guess>\n",
    "      corrected using documents: <correction or None>\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- FIRST LLM ATTEMPT ----\n",
    "    first_guess = llm.invoke(f\"Q: {question}\\nA:\")  # or llm.predict depending on your LLM wrapper\n",
    "    first_text = first_guess.content if isinstance(first_guess, AIMessage) else str(first_guess)\n",
    "\n",
    "    # ---- SIMPLE CONFIDENCE CHECK ----\n",
    "    low_conf = (\n",
    "        \"i am not sure\" in first_text.lower()\n",
    "        or \"sorry\" in first_text.lower()\n",
    "        or len(first_text.strip()) < 30\n",
    "    )\n",
    "\n",
    "    # ---- BUILD A TEMP FAISS DB (demo) ----\n",
    "    docs = [Document(page_content=\"The largest cat is the liger.\")]\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "    # If original answer is confident, return original and indicate no correction needed\n",
    "    if not low_conf:\n",
    "        return (\n",
    "            f\"original answer: {first_text}\\n\"\n",
    "            f\"corrected using documents: None (no correction needed)\"\n",
    "        )\n",
    "\n",
    "    # ---- LOW CONFIDENCE -> RETRIEVE AND FORCE LLM TO USE CONTEXT ----\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    if not retrieved_docs:\n",
    "        corrected_text = \"NO_DOCUMENTS_RETRIEVED\"\n",
    "        return f\"original answer: {first_text}\\ncorrected using documents: {corrected_text}\"\n",
    "\n",
    "    # Compose context by concatenating retrieved documents (trim if too long)\n",
    "    # You can implement token-aware truncation if desired.\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in retrieved_docs)\n",
    "\n",
    "    # Build a strict prompt that tells the model to use only the context\n",
    "    rag_prompt = (\n",
    "        \"You are given context extracted from documents. Use ONLY that context to answer the question.\\n\\n\"\n",
    "        \"CONTEXT:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"QUESTION:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Answer concisely using only the context above. If the answer is not present in the context, say 'I don't know from the provided documents.'\\n\"\n",
    "    )\n",
    "\n",
    "    # Call the LLM with the RAG prompt\n",
    "    corrected = llm.invoke(rag_prompt)\n",
    "    corrected_text = corrected.content if isinstance(corrected, AIMessage) else str(corrected)\n",
    "\n",
    "    # Return formatted output\n",
    "    return (\n",
    "        f\"original answer: {first_text}\\n\"\n",
    "        f\"corrected using documents: {corrected_text}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---- TEST (example) ----\n",
    "if __name__ == \"__main__\":\n",
    "    # ensure llm and embeddings are initialized above before running\n",
    "    response = corrective_rag(\"Which is the largest cat?\")\n",
    "    print(\"\\nFinal Answer:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
